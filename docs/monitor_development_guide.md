# ç›‘æ§æ¨¡å—å¼€å‘æŒ‡å—

æœ¬æŒ‡å—å¸®åŠ©å¼€å‘è€…å¿«é€Ÿåˆ›å»ºæ–°çš„ç½‘ç«™ç›‘æ§æ¨¡å—ï¼Œå®ç°è‡ªåŠ¨åŒ–å…¬å‘Šç›‘æ§å’Œæ¶ˆæ¯æ¨é€ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
qfnu_monitor/
â”œâ”€â”€ core/                    # ç›‘æ§æ¨¡å—æ ¸å¿ƒ
â”‚   â”œâ”€â”€ qfnu_jwc_gg.py      # æ•™åŠ¡å¤„å…¬å‘Šç›‘æ§
â”‚   â”œâ”€â”€ qfnu_jwc_tz.py      # æ•™åŠ¡å¤„é€šçŸ¥ç›‘æ§
â”‚   â”œâ”€â”€ qfnu_library_gg.py  # å›¾ä¹¦é¦†å…¬å‘Šç›‘æ§
â”‚   â””â”€â”€ qfnu_xg_tzgg.py     # å­¦å·¥å¤„é€šçŸ¥ç›‘æ§
â”œâ”€â”€ utils/                   # å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ feishu.py           # é£ä¹¦æ¨é€
â”‚   â”œâ”€â”€ onebot.py           # OneBotæ¨é€
â”‚   â””â”€â”€ logger.py           # æ—¥å¿—ç®¡ç†
â””â”€â”€ main.py                 # ä¸»ç¨‹åºå…¥å£

examples/
â”œâ”€â”€ monitor_template.py     # ç›‘æ§æ¨¡å—æ¨¡æ¿
â””â”€â”€ onebot_example.py       # OneBotä½¿ç”¨ç¤ºä¾‹

docs/
â”œâ”€â”€ onebot_usage.md         # OneBotä½¿ç”¨è¯´æ˜
â””â”€â”€ monitor_development_guide.md  # æœ¬å¼€å‘æŒ‡å—
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ä½¿ç”¨æ¨¡æ¿åˆ›å»ºæ–°ç›‘æ§å™¨

å¤åˆ¶ `examples/monitor_template.py` å¹¶æŒ‰ä»¥ä¸‹æ­¥éª¤ä¿®æ”¹ï¼š

```python
# 1. ä¿®æ”¹ç±»å
class YourWebsiteMonitor(WebsiteMonitorTemplate):
    
    def __init__(self, data_dir="data"):
        super().__init__(data_dir)
        
        # 2. ä¿®æ”¹é…ç½®
        self.url = "https://your-website.com/notices"
        self.base_url = "https://your-website.com/"
        self.site_name = "æ‚¨çš„ç½‘ç«™åç§°"
        self.data_file_prefix = "your_website"

    # 3. é‡å†™è§£ææ–¹æ³•
    def get_notices(self, soup):
        # æ ¹æ®ç½‘ç«™ç»“æ„å®ç°è§£æé€»è¾‘
        pass
```

### 2. æ·»åŠ åˆ°ä¸»ç¨‹åº

åœ¨ `qfnu_monitor/main.py` ä¸­æ³¨å†Œæ–°ç›‘æ§å™¨ï¼š

```python
from qfnu_monitor.core.your_website import YourWebsiteMonitor

def main():
    # ... ç°æœ‰ä»£ç  ...
    
    # æ·»åŠ æ–°ç›‘æ§å™¨
    your_monitor = YourWebsiteMonitor(data_dir=data_dir)
    your_monitor.run()
```

## ğŸ”§ è¯¦ç»†å¼€å‘æ­¥éª¤

### ç¬¬ä¸€æ­¥ï¼šåˆ†æç›®æ ‡ç½‘ç«™

1. **æŸ¥çœ‹ç½‘ç«™ç»“æ„**
   ```bash
   curl -s "https://target-website.com/notices" | head -100
   ```

2. **ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·**
   - æ‰“å¼€ç›®æ ‡ç½‘ç«™
   - æŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…·
   - æŸ¥çœ‹å…¬å‘Šåˆ—è¡¨çš„HTMLç»“æ„
   - è®°å½•é€‰æ‹©å™¨è·¯å¾„

3. **è¯†åˆ«å…³é”®ä¿¡æ¯**
   - å…¬å‘Šæ ‡é¢˜é€‰æ‹©å™¨
   - é“¾æ¥åœ°å€é€‰æ‹©å™¨
   - å‘å¸ƒæ—¥æœŸé€‰æ‹©å™¨
   - åˆ†é¡µæœºåˆ¶ï¼ˆå¦‚æœæœ‰ï¼‰

### ç¬¬äºŒæ­¥ï¼šå®ç°è§£æé€»è¾‘

```python
def get_notices(self, soup):
    notices = []
    
    # æ ¹æ®ç½‘ç«™ç»“æ„ä¿®æ”¹é€‰æ‹©å™¨
    notice_list = soup.select("ul.notice-list li")  # ç¤ºä¾‹é€‰æ‹©å™¨
    
    for item in notice_list:
        try:
            # æå–æ ‡é¢˜
            title_tag = item.select_one("h3 a")
            if not title_tag:
                continue
            title = title_tag.get_text().strip()
            
            # æå–é“¾æ¥
            link = title_tag.get("href")
            if link and not link.startswith("http"):
                link = self.base_url + link.lstrip("/")
            
            # æå–æ—¥æœŸ
            date_tag = item.select_one(".publish-date")
            date = date_tag.get_text().strip() if date_tag else ""
            
            # è¿‡æ»¤å’ŒéªŒè¯
            if title and link:
                notices.append({
                    "title": title,
                    "link": link,
                    "date": date
                })
                
        except Exception as e:
            logger.warning(f"è§£æå…¬å‘Šé¡¹æ—¶å‡ºé”™: {e}")
            continue
    
    return notices
```

### ç¬¬ä¸‰æ­¥ï¼šè‡ªå®šä¹‰æ¶ˆæ¯æ ¼å¼ï¼ˆå¯é€‰ï¼‰

```python
def push_to_feishu(self, new_notices):
    if not new_notices:
        return

    # è‡ªå®šä¹‰æ¶ˆæ¯æ ‡é¢˜å’Œæ ¼å¼
    title = f"ğŸ“ {self.site_name} - {len(new_notices)}æ¡æ–°å…¬å‘Š"
    content = f"ğŸ“… æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"

    for i, notice in enumerate(new_notices, 1):
        content += f"ğŸ“Œ ã€{i}ã€‘{notice['title']}\n"
        content += f"ğŸ•’ {notice['date']}\n"
        content += f"ğŸ”— {notice['link']}\n"
        content += "â”€" * 40 + "\n"

    feishu(title, content)
```

### ç¬¬å››æ­¥ï¼šæµ‹è¯•å’Œè°ƒè¯•

1. **å•å…ƒæµ‹è¯•**
   ```python
   def test_monitor():
       monitor = YourWebsiteMonitor()
       
       # æµ‹è¯•HTMLè·å–
       html = monitor.get_html()
       assert html, "æ— æ³•è·å–HTMLå†…å®¹"
       
       # æµ‹è¯•è§£æ
       soup = monitor.parse_html(html)
       notices = monitor.get_notices(soup)
       assert notices, "æ— æ³•è§£æå…¬å‘Š"
       
       print(f"æˆåŠŸè§£æåˆ°{len(notices)}æ¡å…¬å‘Š")
       for notice in notices[:3]:  # æ˜¾ç¤ºå‰3æ¡
           print(f"- {notice['title']}")
   ```

2. **æ—¥å¿—è°ƒè¯•**
   ```python
   import logging
   logging.basicConfig(level=logging.DEBUG)
   
   monitor = YourWebsiteMonitor()
   monitor.run()
   ```

## ğŸ“‹ å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 1. ç½‘ç«™åçˆ¬è™«æªæ–½

**é—®é¢˜**: è¯·æ±‚è¢«æ‹’ç»æˆ–è¿”å›ç©ºå†…å®¹
**è§£å†³æ–¹æ¡ˆ**:
```python
def get_html(self):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
        'Accept-Encoding': 'gzip, deflate',
        'Connection': 'keep-alive',
    }
    
    # æ·»åŠ å»¶è¿Ÿ
    import time
    time.sleep(1)
    
    response = requests.get(self.url, headers=headers, timeout=15)
    response.encoding = "utf-8"
    return response.text
```

### 2. åŠ¨æ€å†…å®¹åŠ è½½

**é—®é¢˜**: å…¬å‘Šé€šè¿‡JavaScriptåŠ¨æ€åŠ è½½
**è§£å†³æ–¹æ¡ˆ**:
```python
# æ–¹æ¡ˆ1: å¯»æ‰¾APIæ¥å£
def get_notices_from_api(self):
    api_url = "https://website.com/api/notices"
    response = requests.get(api_url)
    data = response.json()
    return data['notices']

# æ–¹æ¡ˆ2: ä½¿ç”¨seleniumï¼ˆéœ€è¦é¢å¤–ä¾èµ–ï¼‰
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

def get_html_with_selenium(self):
    options = Options()
    options.add_argument('--headless')
    driver = webdriver.Chrome(options=options)
    
    driver.get(self.url)
    time.sleep(3)  # ç­‰å¾…å†…å®¹åŠ è½½
    html = driver.page_source
    driver.quit()
    
    return html
```

### 3. å­—ç¬¦ç¼–ç é—®é¢˜

**é—®é¢˜**: ä¸­æ–‡ä¹±ç 
**è§£å†³æ–¹æ¡ˆ**:
```python
def get_html(self):
    response = requests.get(self.url)
    
    # æ–¹æ³•1: è‡ªåŠ¨æ£€æµ‹ç¼–ç 
    response.encoding = response.apparent_encoding
    
    # æ–¹æ³•2: æ‰‹åŠ¨æŒ‡å®šç¼–ç 
    # response.encoding = "gbk"  # æˆ– "utf-8"
    
    return response.text
```

### 4. æ—¥æœŸæ ¼å¼æ ‡å‡†åŒ–

**é—®é¢˜**: ä¸åŒç½‘ç«™æ—¥æœŸæ ¼å¼ä¸ç»Ÿä¸€
**è§£å†³æ–¹æ¡ˆ**:
```python
import re
from datetime import datetime

def normalize_date(self, date_str):
    """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼"""
    if not date_str:
        return ""
    
    # å¸¸è§æ ¼å¼åŒ¹é…
    patterns = [
        (r'(\d{4})-(\d{1,2})-(\d{1,2})', '%Y-%m-%d'),
        (r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', '%Yå¹´%mæœˆ%dæ—¥'),
        (r'(\d{1,2})/(\d{1,2})/(\d{4})', '%m/%d/%Y'),
    ]
    
    for pattern, date_format in patterns:
        if re.match(pattern, date_str):
            try:
                # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
                dt = datetime.strptime(date_str, date_format)
                return dt.strftime('%Y-%m-%d')
            except:
                pass
    
    return date_str  # æ— æ³•è§£ææ—¶è¿”å›åŸå§‹å­—ç¬¦ä¸²
```

## ğŸ› ï¸ é«˜çº§åŠŸèƒ½

### 1. æ”¯æŒåˆ†é¡µ

```python
def get_all_notices(self, max_pages=3):
    """è·å–å¤šé¡µå…¬å‘Š"""
    all_notices = []
    
    for page in range(1, max_pages + 1):
        url = f"{self.url}?page={page}"
        try:
            response = requests.get(url)
            soup = BeautifulSoup(response.text, "html.parser")
            notices = self.get_notices(soup)
            
            if not notices:  # æ²¡æœ‰æ›´å¤šå…¬å‘Š
                break
                
            all_notices.extend(notices)
            time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
            
        except Exception as e:
            logger.error(f"è·å–ç¬¬{page}é¡µæ—¶å‡ºé”™: {e}")
            break
    
    return all_notices
```

### 2. è‡ªå®šä¹‰è¿‡æ»¤è§„åˆ™

```python
def filter_notices(self, notices):
    """è¿‡æ»¤å…¬å‘Š"""
    filtered = []
    
    # å…³é”®è¯è¿‡æ»¤
    keywords = ["é‡è¦", "ç´§æ€¥", "é€šçŸ¥", "å…¬å‘Š"]
    exclude_keywords = ["æµ‹è¯•", "è‰ç¨¿"]
    
    for notice in notices:
        title = notice['title'].lower()
        
        # æ’é™¤ç‰¹å®šå…³é”®è¯
        if any(word in title for word in exclude_keywords):
            continue
            
        # åŒ…å«é‡è¦å…³é”®è¯æˆ–æ‰€æœ‰å…¬å‘Š
        if any(word in title for word in keywords) or not keywords:
            filtered.append(notice)
    
    return filtered
```

### 3. ç›‘æ§é¢‘ç‡æ§åˆ¶

```python
import time
from datetime import datetime, timedelta

class ScheduledMonitor(WebsiteMonitorTemplate):
    def __init__(self, data_dir="data", interval_minutes=30):
        super().__init__(data_dir)
        self.interval = timedelta(minutes=interval_minutes)
        self.last_check = None
    
    def should_run(self):
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰§è¡Œç›‘æ§"""
        if not self.last_check:
            return True
        
        return datetime.now() - self.last_check >= self.interval
    
    def run(self):
        if self.should_run():
            super().run()
            self.last_check = datetime.now()
        else:
            logger.info(f"{self.site_name}: è·ç¦»ä¸Šæ¬¡æ£€æŸ¥ä¸è¶³{self.interval}ï¼Œè·³è¿‡")
```

## ğŸ“ æœ€ä½³å®è·µ

### 1. é”™è¯¯å¤„ç†
- æ€»æ˜¯ä½¿ç”¨ try-except åŒ…è£…ç½‘ç»œè¯·æ±‚
- è®°å½•è¯¦ç»†çš„é”™è¯¯æ—¥å¿—
- å®ç°ä¼˜é›…é™çº§ï¼ˆéƒ¨åˆ†å¤±è´¥ä¸å½±å“æ•´ä½“ï¼‰

### 2. æ€§èƒ½ä¼˜åŒ–
- è®¾ç½®åˆç†çš„è¯·æ±‚è¶…æ—¶æ—¶é—´
- æ·»åŠ è¯·æ±‚é—´éš”é¿å…è¢«å°IP
- ç¼“å­˜ä¸å˜çš„æ•°æ®ï¼ˆå¦‚ç½‘ç«™ç»“æ„ï¼‰

### 3. æ•°æ®ä¸€è‡´æ€§
- ä½¿ç”¨æ ‡é¢˜ä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦
- å®šæœŸæ¸…ç†è¿‡æœŸæ•°æ®
- å¤‡ä»½é‡è¦é€šçŸ¥åˆ°å½’æ¡£æ–‡ä»¶

### 4. å¯ç»´æŠ¤æ€§
- ä½¿ç”¨æ¸…æ™°çš„å˜é‡å‘½å
- æ·»åŠ è¯¦ç»†çš„æ³¨é‡Šå’Œæ–‡æ¡£
- æ¨¡å—åŒ–ä»£ç ä¾¿äºæµ‹è¯•

## ğŸ”„ å®Œæ•´ç¤ºä¾‹

ä»¥ä¸‹æ˜¯ä¸€ä¸ªå®Œæ•´çš„ç›‘æ§å™¨å®ç°ç¤ºä¾‹ï¼š

```python
# qfnu_monitor/core/example_university.py

import requests
import json
import os
import re
from bs4 import BeautifulSoup
from datetime import datetime
from qfnu_monitor.utils.feishu import feishu
from qfnu_monitor.utils.onebot import onebot_send_all
from qfnu_monitor.utils import logger


class ExampleUniversityMonitor:
    """ç¤ºä¾‹å¤§å­¦ç›‘æ§å™¨ - å®Œæ•´å®ç°"""
    
    def __init__(self, data_dir="data"):
        self.url = "https://example-university.edu.cn/notices"
        self.base_url = "https://example-university.edu.cn/"
        self.site_name = "ç¤ºä¾‹å¤§å­¦"
        self.data_dir = data_dir
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs(os.path.join(self.data_dir, "archive"), exist_ok=True)
        
        self.data_file = os.path.join(self.data_dir, "example_university_notices.json")
        self.archive_file = os.path.join(self.data_dir, "archive", "example_university_notices_archive.json")
        self.max_notices = 30

    def get_html(self):
        """è·å–ç½‘é¡µå†…å®¹"""
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        try:
            response = requests.get(self.url, headers=headers, timeout=15)
            response.encoding = "utf-8"
            return response.text
        except Exception as e:
            logger.error(f"è·å–{self.site_name}é¡µé¢å¤±è´¥: {e}")
            raise

    def parse_html(self, html):
        """è§£æHTML"""
        return BeautifulSoup(html, "html.parser")

    def get_notices(self, soup):
        """è§£æå…¬å‘Šåˆ—è¡¨"""
        notices = []
        
        # æ ¹æ®å®é™…ç½‘ç«™ç»“æ„ä¿®æ”¹é€‰æ‹©å™¨
        notice_list = soup.select("div.notice-list .notice-item")
        
        for item in notice_list:
            try:
                # æå–æ ‡é¢˜å’Œé“¾æ¥
                title_tag = item.select_one("h3 a")
                if not title_tag:
                    continue
                
                title = title_tag.get_text().strip()
                link = title_tag.get("href")
                
                # å¤„ç†ç›¸å¯¹é“¾æ¥
                if link and not link.startswith("http"):
                    link = self.base_url + link.lstrip("/")
                
                # æå–æ—¥æœŸ
                date_tag = item.select_one(".notice-date")
                date = self.normalize_date(date_tag.get_text().strip() if date_tag else "")
                
                # éªŒè¯æ•°æ®
                if title and link:
                    notices.append({
                        "title": title,
                        "link": link,
                        "date": date,
                        "created_at": datetime.now().isoformat()
                    })
                    
            except Exception as e:
                logger.warning(f"è§£æ{self.site_name}å…¬å‘Šé¡¹æ—¶å‡ºé”™: {e}")
                continue
        
        return notices

    def normalize_date(self, date_str):
        """æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼"""
        if not date_str:
            return ""
        
        # ç§»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        date_str = re.sub(r'\s+', ' ', date_str.strip())
        
        # å¸¸è§æ—¥æœŸæ ¼å¼å¤„ç†
        patterns = [
            (r'(\d{4})-(\d{1,2})-(\d{1,2})', lambda m: f"{m.group(1)}-{m.group(2).zfill(2)}-{m.group(3).zfill(2)}"),
            (r'(\d{4})å¹´(\d{1,2})æœˆ(\d{1,2})æ—¥', lambda m: f"{m.group(1)}-{m.group(2).zfill(2)}-{m.group(3).zfill(2)}"),
        ]
        
        for pattern, formatter in patterns:
            match = re.search(pattern, date_str)
            if match:
                return formatter(match)
        
        return date_str

    def load_saved_notices(self):
        """åŠ è½½å·²ä¿å­˜çš„å…¬å‘Š"""
        if not os.path.exists(self.data_file) or os.path.getsize(self.data_file) == 0:
            logger.info(f"åˆå§‹åŒ–{self.site_name}å…¬å‘Šè®°å½•æ–‡ä»¶")
            return []

        try:
            with open(self.data_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¯»å–{self.site_name}å…¬å‘Šè®°å½•å¤±è´¥: {e}")
            return []

    def save_notices(self, notices):
        """ä¿å­˜å…¬å‘Š"""
        # ä¿ç•™æœ€æ–°çš„å…¬å‘Š
        latest_notices = notices[-self.max_notices:] if len(notices) > self.max_notices else notices
        
        with open(self.data_file, "w", encoding="utf-8") as f:
            json.dump(latest_notices, f, ensure_ascii=False, indent=2)

        # å½’æ¡£è¶…å‡ºçš„å…¬å‘Š
        if len(notices) > self.max_notices:
            self.archive_notices(notices[:-self.max_notices])

    def archive_notices(self, notices_to_archive):
        """å½’æ¡£å…¬å‘Š"""
        if not notices_to_archive:
            return

        archived_notices = []
        if os.path.exists(self.archive_file):
            try:
                with open(self.archive_file, "r", encoding="utf-8") as f:
                    archived_notices = json.load(f)
            except:
                pass

        all_archived = archived_notices + notices_to_archive

        with open(self.archive_file, "w", encoding="utf-8") as f:
            json.dump(all_archived, f, ensure_ascii=False, indent=2)

        logger.info(f"å·²å½’æ¡£{len(notices_to_archive)}æ¡{self.site_name}å…¬å‘Š")

    def find_new_notices(self, current_notices, saved_notices):
        """æŸ¥æ‰¾æ–°å…¬å‘Š"""
        if not saved_notices:
            return current_notices

        saved_titles = {notice["title"] for notice in saved_notices}
        new_notices = [notice for notice in current_notices if notice["title"] not in saved_titles]
        
        return new_notices

    def push_notifications(self, new_notices):
        """æ¨é€é€šçŸ¥"""
        if not new_notices:
            return

        # æ¨é€åˆ°é£ä¹¦
        try:
            title = f"ğŸ“¢ {self.site_name}æœ‰{len(new_notices)}æ¡æ–°å…¬å‘Š"
            content = f"ğŸ•’ æ£€æŸ¥æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"

            for i, notice in enumerate(new_notices, 1):
                content += f"ğŸ“Œ ã€{i}ã€‘{notice['title']}\n"
                content += f"ğŸ“… {notice['date']}\n"
                content += f"ğŸ”— {notice['link']}\n\n"

            feishu(title, content)
            logger.info(f"{self.site_name}é£ä¹¦æ¨é€æˆåŠŸ")
            
        except Exception as e:
            logger.error(f"{self.site_name}é£ä¹¦æ¨é€å¤±è´¥: {e}")

        # æ¨é€åˆ°OneBot
        try:
            message = f"ğŸ“¢ {self.site_name}æœ‰{len(new_notices)}æ¡æ–°å…¬å‘Š\n\n"

            for i, notice in enumerate(new_notices, 1):
                message += f"ã€{i}ã€‘{notice['title']}\n"
                message += f"ğŸ“… {notice['date']}\n"
                message += f"ğŸ”— {notice['link']}\n\n"

            result = onebot_send_all(message)
            if "error" not in result:
                logger.info(f"{self.site_name}OneBotæ¨é€æˆåŠŸ: {result.get('success_count', 0)} ä¸ªç¾¤ç»„")
            else:
                logger.error(f"{self.site_name}OneBotæ¨é€å¤±è´¥: {result['error']}")
                
        except Exception as e:
            logger.error(f"{self.site_name}OneBotæ¨é€å¤±è´¥: {e}")

    def monitor(self):
        """æ‰§è¡Œç›‘æ§"""
        try:
            # è·å–å½“å‰å…¬å‘Š
            html = self.get_html()
            soup = self.parse_html(html)
            current_notices = self.get_notices(soup)

            if not current_notices:
                logger.warning(f"æœªä»{self.site_name}è·å–åˆ°ä»»ä½•å…¬å‘Š")
                return

            logger.info(f"ä»{self.site_name}è·å–åˆ°{len(current_notices)}æ¡å…¬å‘Š")

            # åŠ è½½å·²ä¿å­˜çš„å…¬å‘Š
            saved_notices = self.load_saved_notices()

            # æŸ¥æ‰¾æ–°å…¬å‘Š
            new_notices = self.find_new_notices(current_notices, saved_notices)

            if new_notices:
                logger.info(f"ä»{self.site_name}å‘ç°{len(new_notices)}æ¡æ–°å…¬å‘Š")
                self.push_notifications(new_notices)
                
                # ä¿å­˜æ›´æ–°åçš„å…¬å‘Š
                all_notices = saved_notices + new_notices
                self.save_notices(all_notices)
            else:
                logger.info(f"{self.site_name}æ²¡æœ‰æ–°å…¬å‘Š")

        except Exception as e:
            logger.error(f"{self.site_name}ç›‘æ§è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")

    def run(self):
        """è¿è¡Œç›‘æ§å™¨"""
        logger.info(f"å¼€å§‹ç›‘æ§{self.site_name}")
        self.monitor()
```

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœåœ¨å¼€å‘è¿‡ç¨‹ä¸­é‡åˆ°é—®é¢˜ï¼š

1. **æŸ¥çœ‹æ—¥å¿—**: æ£€æŸ¥è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
2. **è°ƒè¯•æ¨¡å¼**: ä½¿ç”¨ `logging.DEBUG` çº§åˆ«
3. **æµ‹è¯•ç½‘ç«™**: ç¡®è®¤ç›®æ ‡ç½‘ç«™å¯è®¿é—®
4. **å‚è€ƒç°æœ‰**: æŸ¥çœ‹å·²å®ç°çš„ç›‘æ§å™¨ä»£ç 

å¼€å‘æ„‰å¿«ï¼ğŸ‰