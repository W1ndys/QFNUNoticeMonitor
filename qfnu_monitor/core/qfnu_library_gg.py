import requests
import json
import os
from bs4 import BeautifulSoup
from qfnu_monitor.utils.feishu import feishu
from qfnu_monitor.utils import logger


class QFNUJWCGGMonitor:
    def __init__(self, data_dir="data"):
        self.url = "https://lib.qfnu.edu.cn/index.htm"
        self.base_url = "https://lib.qfnu.edu.cn/"
        self.data_dir = data_dir
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(self.data_dir, exist_ok=True)
        # ç¡®ä¿å½’æ¡£ç›®å½•å­˜åœ¨
        self.archive_dir = os.path.join(self.data_dir, "archive")
        os.makedirs(self.archive_dir, exist_ok=True)
        self.data_file = os.path.join(self.data_dir, "jwc_gg_notices.json")
        self.archive_file = os.path.join(
            self.archive_dir, "jwc_gg_notices_archive.json"
        )
        self.max_notices = 10  # æœ€å¤šä¿ç•™çš„é€šçŸ¥æ•°é‡

    def get_html(self):
        response = requests.get(self.url)
        response.encoding = "utf-8"
        return response.text

    def parse_html(self, html):
        soup = BeautifulSoup(html, "html.parser")
        return soup

    def get_notices(self, soup):
        notices = []
        notice_list = soup.select("ul.n_listxx1 li")

        for item in notice_list:
            title_tag = item.select_one("h2 a")
            if title_tag:
                title = title_tag.get_text().strip()
                link = title_tag.get("href")
                if link and not link.startswith("http"):
                    link = self.base_url + link
                date_tag = item.select_one("h2 span.time")
                date = date_tag.get_text().strip() if date_tag else ""
                notices.append({"title": title, "link": link, "date": date})
        return notices

    def load_saved_notices(self):
        if not os.path.exists(self.data_file) or os.path.getsize(self.data_file) == 0:
            logger.info("åˆå§‹åŒ–æ›²é˜œå¸ˆèŒƒå¤§å­¦å›¾ä¹¦é¦†å…¬å‘Šè®°å½•æ–‡ä»¶")
            return []

        try:
            with open(self.data_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¯»å–æ›²é˜œå¸ˆèŒƒå¤§å­¦å›¾ä¹¦é¦†å…¬å‘Šè®°å½•å¤±è´¥: {e}")
            return []

    def load_archived_notices(self):
        """åŠ è½½å·²å­˜æ¡£çš„å…¬å‘Š"""
        if (
            not os.path.exists(self.archive_file)
            or os.path.getsize(self.archive_file) == 0
        ):
            return []

        try:
            with open(self.archive_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¯»å–æ›²é˜œå¸ˆèŒƒå¤§å­¦å›¾ä¹¦é¦†å…¬å‘Šå­˜æ¡£è®°å½•å¤±è´¥: {e}")
            return []

    def save_notices(self, notices):
        """åªä¿å­˜æœ€æ–°çš„max_noticesæ¡å…¬å‘Š"""
        latest_notices = (
            notices[-self.max_notices :] if len(notices) > self.max_notices else notices
        )
        with open(self.data_file, "w", encoding="utf-8") as f:
            json.dump(latest_notices, f, ensure_ascii=False, indent=2)

        # å¦‚æœæœ‰è¶…è¿‡max_noticesçš„å…¬å‘Šï¼Œå½’æ¡£å¤šä½™çš„å…¬å‘Š
        if len(notices) > self.max_notices:
            self.archive_notices(notices[: -self.max_notices])

    def archive_notices(self, notices_to_archive):
        """å°†å…¬å‘Šå­˜æ¡£"""
        if not notices_to_archive:
            return

        archived_notices = self.load_archived_notices()
        all_archived = archived_notices + notices_to_archive

        with open(self.archive_file, "w", encoding="utf-8") as f:
            json.dump(all_archived, f, ensure_ascii=False, indent=2)

        logger.info(f"å·²å½’æ¡£{len(notices_to_archive)}æ¡å…¬å‘Šåˆ°{self.archive_file}")

    def append_new_notices(self, new_notices):
        """å°†æ–°å…¬å‘Šæ·»åŠ åˆ°å·²ä¿å­˜çš„å…¬å‘Šåˆ—è¡¨ä¸­"""
        saved_notices = self.load_saved_notices()
        all_notices = saved_notices + new_notices
        self.save_notices(all_notices)

    def find_new_notices(self, current_notices, saved_notices):
        if not saved_notices:
            return current_notices

        saved_titles = {notice["title"] for notice in saved_notices}
        return [
            notice for notice in current_notices if notice["title"] not in saved_titles
        ]

    def push_to_feishu(self, new_notices):
        if not new_notices:
            return

        title = f"ğŸ“¢ æ›²é˜œå¸ˆèŒƒå¤§å­¦å›¾ä¹¦é¦†æœ‰{len(new_notices)}æ¡æ–°å…¬å‘Š"
        content = ""

        for i, notice in enumerate(new_notices, 1):
            content += f"ã€{i}ã€‘{notice['title']}\n"
            content += f"ğŸ“… {notice['date']}\n"
            content += f"ğŸ”— {notice['link']}\n\n"

        feishu(title, content)

    def monitor(self):
        try:
            # è·å–å½“å‰å…¬å‘Š
            html = self.get_html()
            soup = self.parse_html(html)
            current_notices = self.get_notices(soup)

            if not current_notices:
                logger.warning("æœªè·å–åˆ°ä»»ä½•å…¬å‘Š")
                return

            # åŠ è½½å·²ä¿å­˜çš„å…¬å‘Š
            saved_notices = self.load_saved_notices()

            # æŸ¥æ‰¾æ–°å…¬å‘Š
            new_notices = self.find_new_notices(current_notices, saved_notices)

            if new_notices:
                logger.info(f"å‘ç°{len(new_notices)}æ¡æ–°å…¬å‘Š")
                self.push_to_feishu(new_notices)
                # æ›´æ–°ä¿å­˜çš„å…¬å‘Šï¼Œæ·»åŠ æ–°å…¬å‘Šè€Œä¸è¦†ç›–å·²æœ‰å…¬å‘Š
                self.append_new_notices(new_notices)
            else:
                logger.info("æ²¡æœ‰æ–°å…¬å‘Š")

        except Exception as e:
            logger.error(f"ç›‘æ§è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")

    def run(self):
        logger.info("å¼€å§‹ç›‘æ§æ›²é˜œå¸ˆèŒƒå¤§å­¦å›¾ä¹¦é¦†å…¬å‘Š")
        self.monitor()


class QFNULibraryGGMonitor:
    def __init__(self, data_dir="data"):
        self.url = "https://lib.qfnu.edu.cn/ggxw/gg.htm"
        self.base_url = "https://lib.qfnu.edu.cn/"
        self.data_dir = data_dir
        # ç¡®ä¿æ•°æ®ç›®å½•å­˜åœ¨
        os.makedirs(self.data_dir, exist_ok=True)
        # ç¡®ä¿å½’æ¡£ç›®å½•å­˜åœ¨
        self.archive_dir = os.path.join(self.data_dir, "archive")
        os.makedirs(self.archive_dir, exist_ok=True)
        self.data_file = os.path.join(self.data_dir, "library_notices.json")
        self.archive_file = os.path.join(
            self.archive_dir, "library_notices_archive.json"
        )
        self.max_notices = 10  # æœ€å¤šä¿ç•™çš„é€šçŸ¥æ•°é‡

    def get_html(self):
        response = requests.get(self.url)
        response.encoding = "utf-8"
        return response.text

    def parse_html(self, html):
        soup = BeautifulSoup(html, "html.parser")
        return soup

    def get_notices(self, soup):
        notices = []
        # å›¾ä¹¦é¦†ç½‘ç«™å…¬å‘Šåˆ—è¡¨é€‰æ‹©å™¨
        notice_list = soup.select("ul.list_box_titu li")

        for item in notice_list:
            title_tag = item.select_one("h5.overfloat-dot")
            if title_tag:
                title = title_tag.get_text().strip()
                link_tag = item.select_one("a")
                link = link_tag.get("href") if link_tag else ""
                if link and not link.startswith("http"):
                    link = self.base_url + link

                # è·å–æ—¥æœŸï¼Œæ—¥æœŸåœ¨time_conä¸­çš„h3(æ—¥)å’Œh6(å¹´æœˆ)
                day_tag = item.select_one("div.time_con h3")
                month_year_tag = item.select_one("div.time_con h6")

                day = day_tag.get_text().strip() if day_tag else ""
                month_year = month_year_tag.get_text().strip() if month_year_tag else ""
                date = f"{month_year}-{day}" if day and month_year else ""

                notices.append({"title": title, "link": link, "date": date})
        return notices

    def load_saved_notices(self):
        if not os.path.exists(self.data_file) or os.path.getsize(self.data_file) == 0:
            logger.info("åˆå§‹åŒ–æ›²é˜œå¸ˆèŒƒå¤§å­¦å›¾ä¹¦é¦†å…¬å‘Šè®°å½•æ–‡ä»¶")
            return []

        try:
            with open(self.data_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¯»å–æ›²é˜œå¸ˆèŒƒå¤§å­¦å›¾ä¹¦é¦†å…¬å‘Šè®°å½•å¤±è´¥: {e}")
            return []

    def load_archived_notices(self):
        """åŠ è½½å·²å­˜æ¡£çš„å…¬å‘Š"""
        if (
            not os.path.exists(self.archive_file)
            or os.path.getsize(self.archive_file) == 0
        ):
            return []

        try:
            with open(self.archive_file, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è¯»å–æ›²é˜œå¸ˆèŒƒå¤§å­¦å›¾ä¹¦é¦†å…¬å‘Šå­˜æ¡£è®°å½•å¤±è´¥: {e}")
            return []

    def save_notices(self, notices):
        """åªä¿å­˜æœ€æ–°çš„max_noticesæ¡å…¬å‘Š"""
        latest_notices = (
            notices[-self.max_notices :] if len(notices) > self.max_notices else notices
        )
        with open(self.data_file, "w", encoding="utf-8") as f:
            json.dump(latest_notices, f, ensure_ascii=False, indent=2)

        # å¦‚æœæœ‰è¶…è¿‡max_noticesçš„å…¬å‘Šï¼Œå½’æ¡£å¤šä½™çš„å…¬å‘Š
        if len(notices) > self.max_notices:
            self.archive_notices(notices[: -self.max_notices])

    def archive_notices(self, notices_to_archive):
        """å°†å…¬å‘Šå­˜æ¡£"""
        if not notices_to_archive:
            return

        archived_notices = self.load_archived_notices()
        all_archived = archived_notices + notices_to_archive

        with open(self.archive_file, "w", encoding="utf-8") as f:
            json.dump(all_archived, f, ensure_ascii=False, indent=2)

        logger.info(f"å·²å½’æ¡£{len(notices_to_archive)}æ¡å…¬å‘Šåˆ°{self.archive_file}")

    def append_new_notices(self, new_notices):
        """å°†æ–°å…¬å‘Šæ·»åŠ åˆ°å·²ä¿å­˜çš„å…¬å‘Šåˆ—è¡¨ä¸­"""
        saved_notices = self.load_saved_notices()
        all_notices = saved_notices + new_notices
        self.save_notices(all_notices)

    def find_new_notices(self, current_notices, saved_notices):
        if not saved_notices:
            return current_notices

        saved_titles = {notice["title"] for notice in saved_notices}
        return [
            notice for notice in current_notices if notice["title"] not in saved_titles
        ]

    def push_to_feishu(self, new_notices):
        if not new_notices:
            return

        title = f"ğŸ“¢ æ›²é˜œå¸ˆèŒƒå¤§å­¦å›¾ä¹¦é¦†æœ‰{len(new_notices)}æ¡æ–°å…¬å‘Š"
        content = ""

        for i, notice in enumerate(new_notices, 1):
            content += f"ã€{i}ã€‘{notice['title']}\n"
            content += f"ğŸ“… {notice['date']}\n"
            content += f"ğŸ”— {notice['link']}\n\n"

        feishu(title, content)

    def monitor(self):
        try:
            # è·å–å½“å‰å…¬å‘Š
            html = self.get_html()
            soup = self.parse_html(html)
            current_notices = self.get_notices(soup)

            if not current_notices:
                logger.warning("æœªè·å–åˆ°ä»»ä½•å…¬å‘Š")
                return

            # åŠ è½½å·²ä¿å­˜çš„å…¬å‘Š
            saved_notices = self.load_saved_notices()

            # æŸ¥æ‰¾æ–°å…¬å‘Š
            new_notices = self.find_new_notices(current_notices, saved_notices)

            if new_notices:
                logger.info(f"å‘ç°{len(new_notices)}æ¡æ–°å…¬å‘Š")
                self.push_to_feishu(new_notices)
                # æ›´æ–°ä¿å­˜çš„å…¬å‘Šï¼Œæ·»åŠ æ–°å…¬å‘Šè€Œä¸è¦†ç›–å·²æœ‰å…¬å‘Š
                self.append_new_notices(new_notices)
            else:
                logger.info("æ²¡æœ‰æ–°å…¬å‘Š")

        except Exception as e:
            logger.error(f"ç›‘æ§è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")

    def run(self):
        logger.info("å¼€å§‹ç›‘æ§æ›²é˜œå¸ˆèŒƒå¤§å­¦å›¾ä¹¦é¦†å…¬å‘Š")
        self.monitor()
